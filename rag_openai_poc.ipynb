{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZZtUJnGlxp9Fe+biSiiXS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/choki0715/lecture/blob/master/rag_openai_poc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NyTOGfO2Ur1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI  # OpenAI로 변경\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "import streamlit as st\n",
        "import tempfile\n",
        "\n",
        "# OpenAI API 키 설정\n",
        "os.environ['OPENAI_API_KEY'] = \"\"  # 실제 API 키로 변경\n",
        "\n",
        "###########################################################################\n",
        "# streamlit UI\n",
        "# webpage 규격 및 디자인\n",
        "st.set_page_config(layout=\"wide\")\n",
        "st.markdown(\"<h1 style='text-align: center; color: gray;'>인구감소 대응 정책을 위한 RAG System</h1>\", unsafe_allow_html=True)\n",
        "st.markdown(\"<h2 style='text-align: center; color: gray;'> Chat with PDFs that you upload </h2>\", unsafe_allow_html=True)\n",
        "st.markdown(\"<h5 style='text-align: center; color: gray;'>OpenAI GPT 기반 (빠름)</h5>\", unsafe_allow_html=True)\n",
        "st.markdown(\"<h3 style='text-align: center; color: gray;'> AIDENTIFY Inc.</h3>\", unsafe_allow_html=True)\n",
        "\n",
        "# streamlit UI\n",
        "###########################################################################\n",
        "\n",
        "def document_data(query, chat_history, vectorstore):\n",
        "    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "\n",
        "    # OpenAI LLM 사용 (빠름)\n",
        "    llm = ChatOpenAI(\n",
        "        model=\"gpt-3.5-turbo\",  # 또는 \"gpt-4o-mini\" (더 저렴)\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    qna = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=vectorstore.as_retriever(),\n",
        "        memory=memory\n",
        "    )\n",
        "\n",
        "    return qna({\"question\": query, \"chat_history\": chat_history})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # OpenAI 임베딩 사용 (매우 빠름)\n",
        "    embeddings = OpenAIEmbeddings(\n",
        "        model=\"text-embedding-3-small\"  # 빠르고 저렴\n",
        "    )\n",
        "\n",
        "    uploaded_files = st.file_uploader(\"Choose a PDF file\", type=\"pdf\", accept_multiple_files=True)\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=100,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "    )\n",
        "\n",
        "    temp_dir = tempfile.TemporaryDirectory()\n",
        "    docs = []\n",
        "\n",
        "    for i, uploaded_file in enumerate(uploaded_files):\n",
        "        st.write(\"filename:\", i, uploaded_file.name)\n",
        "        temp_filepath = os.path.join(temp_dir.name, uploaded_file.name)\n",
        "        with open(temp_filepath, \"wb\") as f:\n",
        "            f.write(uploaded_file.getvalue())\n",
        "\n",
        "        loader = PyPDFLoader(temp_filepath)\n",
        "        docs.extend(loader.load())\n",
        "\n",
        "    if docs:\n",
        "        with st.spinner(\"🔨 벡터 데이터베이스를 구축하는 중...\"):\n",
        "            split_text = text_splitter.split_documents(documents=docs)\n",
        "            vectorstore = FAISS.from_documents(split_text, embedding=embeddings)\n",
        "\n",
        "        st.success(\"✅ PDF 업로드 및 벡터 데이터베이스 구축 완료!\")\n",
        "\n",
        "        st.header(':blue[질문을 입력해 주세요]', divider='rainbow')\n",
        "        prompt = st.chat_input(\"Enter your questions here\")\n",
        "\n",
        "        if \"user_prompt_history\" not in st.session_state:\n",
        "            st.session_state[\"user_prompt_history\"] = []\n",
        "        if \"chat_answers_history\" not in st.session_state:\n",
        "            st.session_state[\"chat_answers_history\"] = []\n",
        "        if \"chat_history\" not in st.session_state:\n",
        "            st.session_state[\"chat_history\"] = []\n",
        "\n",
        "        if prompt:\n",
        "            with st.spinner(\"🤔 답변을 생성하는 중...\"):\n",
        "                output = document_data(\n",
        "                    query=prompt,\n",
        "                    chat_history=st.session_state[\"chat_history\"],\n",
        "                    vectorstore=vectorstore\n",
        "                )\n",
        "\n",
        "                st.session_state[\"chat_answers_history\"].append(output['answer'])\n",
        "                st.session_state[\"user_prompt_history\"].append(prompt)\n",
        "                st.session_state[\"chat_history\"].append((prompt, output['answer']))\n",
        "\n",
        "        if st.session_state[\"chat_answers_history\"]:\n",
        "            for i, j in zip(st.session_state[\"chat_answers_history\"],\n",
        "                          st.session_state[\"user_prompt_history\"]):\n",
        "                message1 = st.chat_message(\"user\")\n",
        "                message1.write(j)\n",
        "                message2 = st.chat_message(\"assistant\")\n",
        "                message2.write(i)"
      ]
    }
  ]
}