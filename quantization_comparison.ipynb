{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/choki0715/lecture/blob/master/quantization_comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bFTT7DU7QQ8"
      },
      "source": [
        "# ğŸš€ Model Quantization Techniques Comparison\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” ë‹¤ì–‘í•œ ì–‘ìí™” ê¸°ë²•ë“¤ì„ ë¹„êµí•©ë‹ˆë‹¤:\n",
        "- **GPTQ** (Generalized Post-Training Quantization)\n",
        "- **AWQ** (Activation-aware Weight Quantization)\n",
        "- **BitsAndBytes** (8-bit and 4-bit quantization)\n",
        "- **Dynamic Quantization** (PyTorch native)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2ljTYOz7QQ9"
      },
      "source": [
        "## ğŸ“¦ 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPtEMxqB7QQ-",
        "outputId": "1fad5583-ae23-43ae-bdbb-eea5c31bbf98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m126.1/126.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.0/121.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31mÃ—\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31mâ•°â”€>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31mÃ—\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31mâ•°â”€>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for autoawq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hâœ… ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ!\n"
          ]
        }
      ],
      "source": [
        "# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "!pip install -q transformers accelerate\n",
        "\n",
        "# GPTQ\n",
        "!pip install -q auto-gptq optimum\n",
        "\n",
        "# AWQ\n",
        "!pip install -q autoawq\n",
        "\n",
        "# BitsAndBytes (8-bit, 4-bit)\n",
        "!pip install -q bitsandbytes\n",
        "\n",
        "print(\"âœ… ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CUDA ë²„ì „ í™•ì¸\n",
        "!nvcc --version\n",
        "\n",
        "# CUDA 11.8ìš© (Colab T4/L4)\n",
        "!pip install https://github.com/casper-hansen/AutoAWQ/releases/download/v0.1.8/autoawq-0.1.8+cu118-cp310-cp310-linux_x86_64.whl\n",
        "\n",
        "# ë˜ëŠ” CUDA 12.1ìš©\n",
        "!pip install https://github.com/casper-hansen/AutoAWQ/releases/download/v0.1.8/autoawq-0.1.8+cu121-cp310-cp310-linux_x86_64.whl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMkeKuwP71a8",
        "outputId": "e2b83271-4805-4f88-b9a6-c28e72225220"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "\u001b[31mERROR: autoawq-0.1.8+cu118-cp310-cp310-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: autoawq-0.1.8+cu121-cp310-cp310-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2foAkILn7QQ-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "import time\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GPTQConfig, BitsAndBytesConfig\n",
        "from awq import AutoAWQForCausalLM\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# GPU ì‚¬ìš© ê°€ëŠ¥ í™•ì¸\n",
        "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y686olk7QQ-"
      },
      "source": [
        "## ğŸ”§ 2. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCLe5L0u7QQ_"
      },
      "outputs": [],
      "source": [
        "def get_model_size(model):\n",
        "    \"\"\"ëª¨ë¸ í¬ê¸°ë¥¼ MB ë‹¨ìœ„ë¡œ ê³„ì‚°\"\"\"\n",
        "    param_size = 0\n",
        "    buffer_size = 0\n",
        "\n",
        "    for param in model.parameters():\n",
        "        param_size += param.nelement() * param.element_size()\n",
        "\n",
        "    for buffer in model.buffers():\n",
        "        buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "    size_mb = (param_size + buffer_size) / 1024 / 1024\n",
        "    return size_mb\n",
        "\n",
        "def get_gpu_memory():\n",
        "    \"\"\"í˜„ì¬ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.cuda.memory_allocated() / 1024**2  # MB\n",
        "    return 0\n",
        "\n",
        "def cleanup():\n",
        "    \"\"\"ë©”ëª¨ë¦¬ ì •ë¦¬\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "def test_inference(model, tokenizer, prompt=\"Hello, how are you?\"):\n",
        "    \"\"\"ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸\"\"\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n",
        "    end_time = time.time()\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return {\n",
        "        \"text\": generated_text,\n",
        "        \"time\": end_time - start_time\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdmvnJo47QQ_"
      },
      "source": [
        "## ğŸ¯ 3. í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ì„ íƒ\n",
        "\n",
        "ì‘ì€ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxABpaH47QQ_"
      },
      "outputs": [],
      "source": [
        "# í…ŒìŠ¤íŠ¸ìš© ëª¨ë¸ (ì‘ì€ ëª¨ë¸ ì‚¬ìš©)\n",
        "model_id = \"facebook/opt-125m\"  # 125M íŒŒë¼ë¯¸í„° ëª¨ë¸\n",
        "# model_id = \"facebook/opt-350m\"  # ë” í° ëª¨ë¸ í…ŒìŠ¤íŠ¸ì‹œ\n",
        "# model_id = \"facebook/opt-1.3b\"  # 1.3B ëª¨ë¸ (GPU ë©”ëª¨ë¦¬ ì¶©ë¶„ì‹œ)\n",
        "\n",
        "print(f\"Selected Model: {model_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37SJ3WjF7QQ_"
      },
      "source": [
        "## ğŸ“Š 4. ê¸°ì¤€ ëª¨ë¸ (FP32/FP16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osHw7rns7QQ_"
      },
      "outputs": [],
      "source": [
        "print(\"ğŸ“Š Loading Base Model...\")\n",
        "cleanup()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# FP16 ëª¨ë¸ ë¡œë“œ (ë©”ëª¨ë¦¬ íš¨ìœ¨)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "base_size = get_model_size(base_model)\n",
        "base_memory = get_gpu_memory()\n",
        "base_result = test_inference(base_model, tokenizer)\n",
        "\n",
        "print(f\"âœ… Base Model (FP16):\")\n",
        "print(f\"   - Size: {base_size:.2f} MB\")\n",
        "print(f\"   - GPU Memory: {base_memory:.2f} MB\")\n",
        "print(f\"   - Inference Time: {base_result['time']:.3f} seconds\")\n",
        "print(f\"   - Output: {base_result['text'][:100]}...\")\n",
        "\n",
        "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
        "del base_model\n",
        "cleanup()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBOU5bVr7QQ_"
      },
      "source": [
        "## ğŸ”µ 5. BitsAndBytes ì–‘ìí™” (8-bit & 4-bit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgnpy_S77QQ_"
      },
      "outputs": [],
      "source": [
        "# 8-bit ì–‘ìí™”\n",
        "print(\"ğŸ”µ Loading 8-bit Quantized Model...\")\n",
        "cleanup()\n",
        "\n",
        "bnb_config_8bit = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_8bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config_8bit,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "bit8_size = get_model_size(model_8bit)\n",
        "bit8_memory = get_gpu_memory()\n",
        "bit8_result = test_inference(model_8bit, tokenizer)\n",
        "\n",
        "print(f\"âœ… 8-bit Quantized Model:\")\n",
        "print(f\"   - Size: {bit8_size:.2f} MB\")\n",
        "print(f\"   - Size Reduction: {(1 - bit8_size/base_size)*100:.1f}%\")\n",
        "print(f\"   - GPU Memory: {bit8_memory:.2f} MB\")\n",
        "print(f\"   - Inference Time: {bit8_result['time']:.3f} seconds\")\n",
        "\n",
        "del model_8bit\n",
        "cleanup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlVMN3nD7QRA"
      },
      "outputs": [],
      "source": [
        "# 4-bit ì–‘ìí™”\n",
        "print(\"ğŸ”µ Loading 4-bit Quantized Model...\")\n",
        "cleanup()\n",
        "\n",
        "bnb_config_4bit = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "model_4bit = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config_4bit,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "bit4_size = get_model_size(model_4bit)\n",
        "bit4_memory = get_gpu_memory()\n",
        "bit4_result = test_inference(model_4bit, tokenizer)\n",
        "\n",
        "print(f\"âœ… 4-bit Quantized Model:\")\n",
        "print(f\"   - Size: {bit4_size:.2f} MB\")\n",
        "print(f\"   - Size Reduction: {(1 - bit4_size/base_size)*100:.1f}%\")\n",
        "print(f\"   - GPU Memory: {bit4_memory:.2f} MB\")\n",
        "print(f\"   - Inference Time: {bit4_result['time']:.3f} seconds\")\n",
        "\n",
        "del model_4bit\n",
        "cleanup()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs5WHCaf7QRA"
      },
      "source": [
        "## ğŸŸ¢ 6. GPTQ ì–‘ìí™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbkF_FQ87QRA"
      },
      "outputs": [],
      "source": [
        "print(\"ğŸŸ¢ Loading GPTQ Quantized Model...\")\n",
        "cleanup()\n",
        "\n",
        "# GPTQ ì„¤ì •\n",
        "gptq_config = GPTQConfig(\n",
        "    bits=4,\n",
        "    dataset=\"c4\",  # ë˜ëŠ” ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ ì‚¬ìš©\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# ì‹¤ì œ ì–‘ìí™” ìˆ˜í–‰ (ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤)\n",
        "print(\"   Quantizing model (this may take a while)...\")\n",
        "model_gptq = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=gptq_config\n",
        ")\n",
        "\n",
        "gptq_size = get_model_size(model_gptq)\n",
        "gptq_memory = get_gpu_memory()\n",
        "gptq_result = test_inference(model_gptq, tokenizer)\n",
        "\n",
        "print(f\"âœ… GPTQ Quantized Model:\")\n",
        "print(f\"   - Size: {gptq_size:.2f} MB\")\n",
        "print(f\"   - Size Reduction: {(1 - gptq_size/base_size)*100:.1f}%\")\n",
        "print(f\"   - GPU Memory: {gptq_memory:.2f} MB\")\n",
        "print(f\"   - Inference Time: {gptq_result['time']:.3f} seconds\")\n",
        "\n",
        "del model_gptq\n",
        "cleanup()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70eSO-wi7QRA"
      },
      "source": [
        "## ğŸŸ¡ 7. AWQ ì–‘ìí™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-nBVTa67QRA"
      },
      "outputs": [],
      "source": [
        "print(\"ğŸŸ¡ Loading AWQ Quantized Model...\")\n",
        "cleanup()\n",
        "\n",
        "# AWQ ëª¨ë¸ ë¡œë“œ\n",
        "# ì£¼ì˜: AWQëŠ” ì‚¬ì „ ì–‘ìí™”ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê±°ë‚˜ ì§ì ‘ ì–‘ìí™”í•´ì•¼ í•©ë‹ˆë‹¤\n",
        "try:\n",
        "    # AWQ ëª¨ë¸ ê²½ë¡œ (ì‚¬ì „ ì–‘ìí™”ëœ ëª¨ë¸ ì‚¬ìš©)\n",
        "    awq_model_path = model_id  # ë˜ëŠ” AWQ ì–‘ìí™”ëœ ëª¨ë¸ ê²½ë¡œ\n",
        "\n",
        "    # AWQ ì–‘ìí™” ì„¤ì •\n",
        "    quant_config = {\n",
        "        \"zero_point\": True,\n",
        "        \"q_group_size\": 128,\n",
        "        \"w_bit\": 4,\n",
        "        \"version\": \"GEMM\"\n",
        "    }\n",
        "\n",
        "    # ëª¨ë¸ ë¡œë“œ\n",
        "    model_awq = AutoAWQForCausalLM.from_pretrained(\n",
        "        awq_model_path,\n",
        "        safetensors=False\n",
        "    )\n",
        "\n",
        "    # ì–‘ìí™” ìˆ˜í–‰ (í•„ìš”í•œ ê²½ìš°)\n",
        "    print(\"   Quantizing with AWQ...\")\n",
        "    model_awq.quantize(\n",
        "        tokenizer,\n",
        "        quant_config=quant_config\n",
        "    )\n",
        "\n",
        "    awq_size = get_model_size(model_awq.model)\n",
        "    awq_memory = get_gpu_memory()\n",
        "\n",
        "    print(f\"âœ… AWQ Quantized Model:\")\n",
        "    print(f\"   - Size: {awq_size:.2f} MB\")\n",
        "    print(f\"   - Size Reduction: {(1 - awq_size/base_size)*100:.1f}%\")\n",
        "    print(f\"   - GPU Memory: {awq_memory:.2f} MB\")\n",
        "\n",
        "    del model_awq\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ AWQ ì–‘ìí™” ì‹¤íŒ¨: {e}\")\n",
        "    print(\"   AWQëŠ” íŠ¹ì • ëª¨ë¸ì—ë§Œ ì§€ì›ë˜ê±°ë‚˜ ì‚¬ì „ ì–‘ìí™”ëœ ëª¨ë¸ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "cleanup()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3LjK4nw7QRA"
      },
      "source": [
        "## ğŸ”´ 8. Dynamic Quantization (PyTorch Native)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCujeba-7QRA"
      },
      "outputs": [],
      "source": [
        "print(\"ğŸ”´ Applying Dynamic Quantization...\")\n",
        "cleanup()\n",
        "\n",
        "# ê¸°ë³¸ ëª¨ë¸ ë‹¤ì‹œ ë¡œë“œ\n",
        "base_model_cpu = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float32\n",
        ").to('cpu')  # Dynamic quantizationì€ CPUì—ì„œ ë™ì‘\n",
        "\n",
        "# Dynamic Quantization ì ìš©\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    base_model_cpu,\n",
        "    {torch.nn.Linear},  # Linear ë ˆì´ì–´ë§Œ ì–‘ìí™”\n",
        "    dtype=torch.qint8\n",
        ")\n",
        "\n",
        "dynamic_size = get_model_size(quantized_model)\n",
        "\n",
        "print(f\"âœ… Dynamic Quantized Model:\")\n",
        "print(f\"   - Size: {dynamic_size:.2f} MB\")\n",
        "print(f\"   - Size Reduction: {(1 - dynamic_size/base_size)*100:.1f}%\")\n",
        "print(f\"   - Note: Dynamic quantization runs on CPU\")\n",
        "\n",
        "del base_model_cpu, quantized_model\n",
        "cleanup()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FP9yet37QRA"
      },
      "source": [
        "## ğŸ“ˆ 9. ê²°ê³¼ ë¹„êµ ë° ìš”ì•½"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEO5vwKz7QRA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ê²°ê³¼ ë°ì´í„° (ì‹¤ì œ ì‹¤í–‰ ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸ í•„ìš”)\n",
        "results = {\n",
        "    \"Method\": [\"Base (FP16)\", \"BitsAndBytes 8-bit\", \"BitsAndBytes 4-bit\", \"GPTQ 4-bit\"],\n",
        "    \"Model Size (MB)\": [base_size, bit8_size, bit4_size, gptq_size],\n",
        "    \"Size Reduction (%)\": [0,\n",
        "                           (1 - bit8_size/base_size)*100,\n",
        "                           (1 - bit4_size/base_size)*100,\n",
        "                           (1 - gptq_size/base_size)*100],\n",
        "    \"GPU Memory (MB)\": [base_memory, bit8_memory, bit4_memory, gptq_memory],\n",
        "    \"Inference Time (s)\": [base_result['time'],\n",
        "                          bit8_result['time'],\n",
        "                          bit4_result['time'],\n",
        "                          gptq_result['time']]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "df = df.round(2)\n",
        "\n",
        "print(\"\\nğŸ“Š === Quantization Methods Comparison ===\")\n",
        "print(df.to_string(index=False))\n",
        "\n",
        "# ì‹œê°í™”\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "# Model Size\n",
        "axes[0, 0].bar(df['Method'], df['Model Size (MB)'], color=['gray', 'blue', 'green', 'orange'])\n",
        "axes[0, 0].set_title('Model Size Comparison')\n",
        "axes[0, 0].set_ylabel('Size (MB)')\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Size Reduction\n",
        "axes[0, 1].bar(df['Method'], df['Size Reduction (%)'], color=['gray', 'blue', 'green', 'orange'])\n",
        "axes[0, 1].set_title('Size Reduction')\n",
        "axes[0, 1].set_ylabel('Reduction (%)')\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# GPU Memory\n",
        "axes[1, 0].bar(df['Method'], df['GPU Memory (MB)'], color=['gray', 'blue', 'green', 'orange'])\n",
        "axes[1, 0].set_title('GPU Memory Usage')\n",
        "axes[1, 0].set_ylabel('Memory (MB)')\n",
        "axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Inference Time\n",
        "axes[1, 1].bar(df['Method'], df['Inference Time (s)'], color=['gray', 'blue', 'green', 'orange'])\n",
        "axes[1, 1].set_title('Inference Time')\n",
        "axes[1, 1].set_ylabel('Time (seconds)')\n",
        "axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HyoIu7k7QRB"
      },
      "source": [
        "## ğŸ’¡ 10. ì–‘ìí™” ë°©ë²•ë³„ íŠ¹ì§• ë° ì‚¬ìš© ê°€ì´ë“œ\n",
        "\n",
        "### ğŸ“Œ **BitsAndBytes**\n",
        "- **ì¥ì **: ì„¤ì •ì´ ê°„ë‹¨í•˜ê³  ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥\n",
        "- **ë‹¨ì **: ì¶”ë¡  ì†ë„ê°€ ìƒëŒ€ì ìœ¼ë¡œ ëŠë¦¼\n",
        "- **ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤**: ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘, ë©”ëª¨ë¦¬ ì œì•½ì´ ìˆëŠ” í™˜ê²½\n",
        "\n",
        "### ğŸ“Œ **GPTQ**\n",
        "- **ì¥ì **: ìš°ìˆ˜í•œ ì••ì¶•ë¥ ê³¼ í’ˆì§ˆ ìœ ì§€\n",
        "- **ë‹¨ì **: ì–‘ìí™” ê³¼ì •ì´ ì˜¤ë˜ ê±¸ë¦¼ (ìº˜ë¦¬ë¸Œë ˆì´ì…˜ í•„ìš”)\n",
        "- **ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤**: í”„ë¡œë•ì…˜ ë°°í¬, í’ˆì§ˆì´ ì¤‘ìš”í•œ ê²½ìš°\n",
        "\n",
        "### ğŸ“Œ **AWQ**\n",
        "- **ì¥ì **: Activationì„ ê³ ë ¤í•œ ì–‘ìí™”ë¡œ í’ˆì§ˆ ìš°ìˆ˜\n",
        "- **ë‹¨ì **: ëª¨ë“  ëª¨ë¸ì„ ì§€ì›í•˜ì§€ ì•ŠìŒ\n",
        "- **ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤**: ì§€ì›ë˜ëŠ” ëª¨ë¸ì—ì„œ ìµœì  ì„±ëŠ¥ í•„ìš”ì‹œ\n",
        "\n",
        "### ğŸ“Œ **Dynamic Quantization**\n",
        "- **ì¥ì **: ì¶”ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆí•„ìš”\n",
        "- **ë‹¨ì **: GPU ì§€ì› ì œí•œì \n",
        "- **ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤**: CPU ì¶”ë¡ , ê°„ë‹¨í•œ ì–‘ìí™” í•„ìš”ì‹œ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZfbc5Xr7QRB"
      },
      "source": [
        "## ğŸ¯ 11. ì‹¤ì „ íŒ\n",
        "\n",
        "1. **ëª¨ë¸ í¬ê¸°ë³„ ì¶”ì²œ ì–‘ìí™”**:\n",
        "   - 1B ì´í•˜: BitsAndBytes 4-bit\n",
        "   - 1B-7B: GPTQ ë˜ëŠ” AWQ\n",
        "   - 7B ì´ìƒ: AWQ (ê°€ëŠ¥í•œ ê²½ìš°) ë˜ëŠ” GPTQ\n",
        "\n",
        "2. **í•˜ë“œì›¨ì–´ë³„ ì¶”ì²œ**:\n",
        "   - Consumer GPU (8GB ì´í•˜): 4-bit ì–‘ìí™” í•„ìˆ˜\n",
        "   - Mid-range GPU (16GB): 8-bit ë˜ëŠ” 4-bit\n",
        "   - High-end GPU (24GB+): FP16 ë˜ëŠ” 8-bit\n",
        "\n",
        "3. **í’ˆì§ˆ vs ì†ë„ íŠ¸ë ˆì´ë“œì˜¤í”„**:\n",
        "   - ìµœê³  í’ˆì§ˆ: FP16 > 8-bit > GPTQ/AWQ > BitsAndBytes 4-bit\n",
        "   - ìµœê³  ì†ë„: AWQ > GPTQ > BitsAndBytes\n",
        "   - ìµœì†Œ ë©”ëª¨ë¦¬: 4-bit ì–‘ìí™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGoFpler7QRB"
      },
      "outputs": [],
      "source": [
        "# ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬\n",
        "cleanup()\n",
        "print(\"\\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ! ë©”ëª¨ë¦¬ê°€ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
      ]
    }
  ]
}