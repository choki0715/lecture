{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "authorship_tag": "ABX9TyP0uLe+RIn8FhvSIpArIDdm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/choki0715/lecture/blob/master/MLP_MNIST_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkZoP2JHvoFD",
        "outputId": "67f37da4-9f00-43a6-8e16-df6017ba921b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 31.7MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.06MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 9.78MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 17.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
            "  (relu1): ReLU()\n",
            "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (relu2): ReLU()\n",
            "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n",
            "\n",
            "총 파라미터 수: 535,818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# 디바이스 설정\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# 2개의 은닉층을 가진 MLP 모델 정의\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size=784, hidden1_size=512, hidden2_size=256, num_classes=10):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden1_size)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(hidden2_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 입력 이미지를 1차원으로 펼치기\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # 첫 번째 은닉층\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        # 두 번째 은닉층\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        # 출력층\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "batch_size = 128\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "\n",
        "# 데이터 전처리\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST 평균과 표준편차\n",
        "])\n",
        "\n",
        "# MNIST 데이터셋 로드\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# 모델 초기화\n",
        "model = MLP().to(device)\n",
        "print(model)\n",
        "print(f'\\n총 파라미터 수: {sum(p.numel() for p in model.parameters()):,}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 손실 함수와 옵티마이저\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 학습 함수\n",
        "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # 기울기 초기화\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 순전파\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # 역전파\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # 통계 계산\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = output.max(1)\n",
        "        total += target.size(0)\n",
        "        correct += predicted.eq(target).sum().item()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
        "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "    accuracy = 100. * correct / total\n",
        "    avg_loss = train_loss / len(train_loader)\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# 테스트 함수\n",
        "def test(model, device, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()\n",
        "            _, predicted = output.max(1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    accuracy = 100. * correct / total\n",
        "\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, '\n",
        "          f'Accuracy: {correct}/{total} ({accuracy:.2f}%)\\n')\n",
        "\n",
        "    return test_loss, accuracy\n",
        "\n",
        "# 학습 실행\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "print('학습 시작...\\n')\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_loss, train_acc = train(model, device, train_loader, optimizer, criterion, epoch)\n",
        "    test_loss, test_acc = test(model, device, test_loader, criterion)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkIv1dLNvyFF",
        "outputId": "85e978aa-6e56-4c8c-e4f9-f5b6182c29c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습 시작...\n",
            "\n",
            "Epoch: 1 [0/60000 (0%)]\tLoss: 2.295684\n",
            "Epoch: 1 [12800/60000 (21%)]\tLoss: 0.226062\n",
            "Epoch: 1 [25600/60000 (43%)]\tLoss: 0.222512\n",
            "Epoch: 1 [38400/60000 (64%)]\tLoss: 0.117284\n",
            "Epoch: 1 [51200/60000 (85%)]\tLoss: 0.095173\n",
            "\n",
            "Test set: Average loss: 0.1085, Accuracy: 9646/10000 (96.46%)\n",
            "\n",
            "Epoch: 2 [0/60000 (0%)]\tLoss: 0.061509\n",
            "Epoch: 2 [12800/60000 (21%)]\tLoss: 0.022178\n",
            "Epoch: 2 [25600/60000 (43%)]\tLoss: 0.042374\n",
            "Epoch: 2 [38400/60000 (64%)]\tLoss: 0.036901\n",
            "Epoch: 2 [51200/60000 (85%)]\tLoss: 0.078590\n",
            "\n",
            "Test set: Average loss: 0.0972, Accuracy: 9705/10000 (97.05%)\n",
            "\n",
            "Epoch: 3 [0/60000 (0%)]\tLoss: 0.079712\n",
            "Epoch: 3 [12800/60000 (21%)]\tLoss: 0.047117\n",
            "Epoch: 3 [25600/60000 (43%)]\tLoss: 0.017512\n",
            "Epoch: 3 [38400/60000 (64%)]\tLoss: 0.020257\n",
            "Epoch: 3 [51200/60000 (85%)]\tLoss: 0.035660\n",
            "\n",
            "Test set: Average loss: 0.0720, Accuracy: 9768/10000 (97.68%)\n",
            "\n",
            "Epoch: 4 [0/60000 (0%)]\tLoss: 0.044682\n",
            "Epoch: 4 [12800/60000 (21%)]\tLoss: 0.079537\n",
            "Epoch: 4 [25600/60000 (43%)]\tLoss: 0.018016\n",
            "Epoch: 4 [38400/60000 (64%)]\tLoss: 0.037837\n",
            "Epoch: 4 [51200/60000 (85%)]\tLoss: 0.126574\n",
            "\n",
            "Test set: Average loss: 0.0892, Accuracy: 9732/10000 (97.32%)\n",
            "\n",
            "Epoch: 5 [0/60000 (0%)]\tLoss: 0.061852\n",
            "Epoch: 5 [12800/60000 (21%)]\tLoss: 0.001844\n",
            "Epoch: 5 [25600/60000 (43%)]\tLoss: 0.006158\n",
            "Epoch: 5 [38400/60000 (64%)]\tLoss: 0.067829\n",
            "Epoch: 5 [51200/60000 (85%)]\tLoss: 0.010290\n",
            "\n",
            "Test set: Average loss: 0.0808, Accuracy: 9757/10000 (97.57%)\n",
            "\n",
            "Epoch: 6 [0/60000 (0%)]\tLoss: 0.033050\n",
            "Epoch: 6 [12800/60000 (21%)]\tLoss: 0.017964\n",
            "Epoch: 6 [25600/60000 (43%)]\tLoss: 0.129066\n",
            "Epoch: 6 [38400/60000 (64%)]\tLoss: 0.017965\n",
            "Epoch: 6 [51200/60000 (85%)]\tLoss: 0.061271\n",
            "\n",
            "Test set: Average loss: 0.0841, Accuracy: 9770/10000 (97.70%)\n",
            "\n",
            "Epoch: 7 [0/60000 (0%)]\tLoss: 0.063744\n",
            "Epoch: 7 [12800/60000 (21%)]\tLoss: 0.012940\n",
            "Epoch: 7 [25600/60000 (43%)]\tLoss: 0.045138\n",
            "Epoch: 7 [38400/60000 (64%)]\tLoss: 0.012290\n",
            "Epoch: 7 [51200/60000 (85%)]\tLoss: 0.118009\n",
            "\n",
            "Test set: Average loss: 0.0847, Accuracy: 9777/10000 (97.77%)\n",
            "\n",
            "Epoch: 8 [0/60000 (0%)]\tLoss: 0.009727\n",
            "Epoch: 8 [12800/60000 (21%)]\tLoss: 0.005862\n",
            "Epoch: 8 [25600/60000 (43%)]\tLoss: 0.043743\n",
            "Epoch: 8 [38400/60000 (64%)]\tLoss: 0.096017\n",
            "Epoch: 8 [51200/60000 (85%)]\tLoss: 0.018443\n",
            "\n",
            "Test set: Average loss: 0.0823, Accuracy: 9789/10000 (97.89%)\n",
            "\n",
            "Epoch: 9 [0/60000 (0%)]\tLoss: 0.016029\n",
            "Epoch: 9 [12800/60000 (21%)]\tLoss: 0.017346\n",
            "Epoch: 9 [25600/60000 (43%)]\tLoss: 0.014246\n",
            "Epoch: 9 [38400/60000 (64%)]\tLoss: 0.035608\n",
            "Epoch: 9 [51200/60000 (85%)]\tLoss: 0.005476\n",
            "\n",
            "Test set: Average loss: 0.0794, Accuracy: 9801/10000 (98.01%)\n",
            "\n",
            "Epoch: 10 [0/60000 (0%)]\tLoss: 0.005480\n",
            "Epoch: 10 [12800/60000 (21%)]\tLoss: 0.004944\n",
            "Epoch: 10 [25600/60000 (43%)]\tLoss: 0.001463\n",
            "Epoch: 10 [38400/60000 (64%)]\tLoss: 0.002549\n",
            "Epoch: 10 [51200/60000 (85%)]\tLoss: 0.000610\n",
            "\n",
            "Test set: Average loss: 0.0747, Accuracy: 9812/10000 (98.12%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 결과 시각화\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# 손실 그래프\n",
        "ax1.plot(range(1, num_epochs + 1), train_losses, label='Train Loss', marker='o')\n",
        "ax1.plot(range(1, num_epochs + 1), test_losses, label='Test Loss', marker='s')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Training and Test Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "# 정확도 그래프\n",
        "ax2.plot(range(1, num_epochs + 1), train_accuracies, label='Train Accuracy', marker='o')\n",
        "ax2.plot(range(1, num_epochs + 1), test_accuracies, label='Test Accuracy', marker='s')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy (%)')\n",
        "ax2.set_title('Training and Test Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/mnt/user-data/outputs/training_results.png', dpi=300, bbox_inches='tight')\n",
        "print('학습 결과 그래프가 저장되었습니다.')\n",
        "\n",
        "# 모델 저장\n",
        "torch.save(model.state_dict(), '/mnt/user-data/outputs/mnist_mlp.pth')\n",
        "print('모델이 저장되었습니다.')\n",
        "\n",
        "# 예측 샘플 시각화\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # 테스트 데이터에서 샘플 추출\n",
        "    data_iter = iter(test_loader)\n",
        "    images, labels = next(data_iter)\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    # 예측\n",
        "    outputs = model(images)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    # 처음 10개 샘플 시각화\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
        "    for idx, ax in enumerate(axes.flat):\n",
        "        img = images[idx].cpu().squeeze()\n",
        "        ax.imshow(img, cmap='gray')\n",
        "        ax.set_title(f'예측: {predicted[idx].item()}\\n실제: {labels[idx].item()}',\n",
        "                    color='green' if predicted[idx] == labels[idx] else 'red')\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/mnt/user-data/outputs/predictions.png', dpi=300, bbox_inches='tight')\n",
        "    print('예측 결과 시각화가 저장되었습니다.')\n",
        "\n",
        "print(f'\\n최종 테스트 정확도: {test_accuracies[-1]:.2f}%')"
      ],
      "metadata": {
        "id": "DyhZf_Yzv9Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 예측 결과 시각화 (요청하신 코드 적용)\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"예측 결과 시각화 중...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model.eval()\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = next(dataiter)\n",
        "images = images.to(device)  # Move images to the device\n",
        "\n",
        "# get sample outputs\n",
        "output = model(images)\n",
        "# convert output probabilities to predicted class\n",
        "_, preds = torch.max(output, 1)\n",
        "# prep images for display\n",
        "images = images.cpu().numpy()  # Move images back to CPU for numpy conversion\n",
        "\n",
        "# plot the images in the batch, along with predicted and true labels\n",
        "fig = plt.figure(figsize=(16, 16))\n",
        "for idx in np.arange(36):\n",
        "    ax = fig.add_subplot(6, 6, idx+1, xticks=[], yticks=[])\n",
        "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
        "    ax.set_title(\"{} ({})\".format(str(preds[idx].item()), str(labels[idx].item())),\n",
        "                 color=(\"green\" if preds[idx]==labels[idx] else \"red\"),\n",
        "                 fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.suptitle('Prediction Results (Predicted (True Label))\\nGreen: Correct, Red: Incorrect',\n",
        "             fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/mnt/user-data/outputs/predictions_visualization.png', dpi=150, bbox_inches='tight')\n",
        "print(\"예측 결과 시각화가 저장되었습니다: predictions_visualization.png\")\n",
        "\n",
        "# 정확도 통계\n",
        "correct_predictions = (preds == labels.to(device)).sum().item()\n",
        "total_predictions = len(labels)\n",
        "accuracy_batch = 100. * correct_predictions / total_predictions\n",
        "print(f\"\\n이 배치의 정확도: {correct_predictions}/{total_predictions} = {accuracy_batch:.2f}%\")\n",
        "\n",
        "# 오분류된 샘플 분석\n",
        "misclassified = []\n",
        "for idx in range(len(preds)):\n",
        "    if preds[idx] != labels[idx]:\n",
        "        misclassified.append({\n",
        "            'index': idx,\n",
        "            'predicted': preds[idx].item(),\n",
        "            'true': labels[idx].item()\n",
        "        })\n",
        "\n",
        "if misclassified:\n",
        "    print(f\"\\n오분류된 샘플: {len(misclassified)}개\")\n",
        "    print(\"처음 5개의 오분류:\")\n",
        "    for i, mis in enumerate(misclassified[:5]):\n",
        "        print(f\"  {i+1}. 인덱스 {mis['index']}: 예측={mis['predicted']}, 실제={mis['true']}\")\n",
        "else:\n",
        "    print(\"\\n이 배치에서 모든 예측이 정확합니다! 🎉\")\n",
        "\n",
        "# 혼동 행렬 생성 (전체 테스트 데이터)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"전체 테스트 데이터에 대한 혼동 행렬 생성 중...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        data = data.to(device)\n",
        "        output = model(data)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(target.numpy())\n",
        "\n",
        "# 혼동 행렬\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# 혼동 행렬 시각화\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=range(10), yticklabels=range(10))\n",
        "plt.title('Confusion Matrix', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/mnt/user-data/outputs/confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
        "print(\"혼동 행렬이 저장되었습니다: confusion_matrix.png\")\n",
        "\n",
        "# 분류 리포트\n",
        "print(\"\\n분류 리포트:\")\n",
        "print(classification_report(all_labels, all_preds,\n",
        "                          target_names=[str(i) for i in range(10)]))\n",
        "\n",
        "# 모델 저장\n",
        "torch.save(model.state_dict(), '/mnt/user-data/outputs/mnist_cnn_model.pth')\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"모델이 저장되었습니다: mnist_cnn_model.pth\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\n✅ 모든 작업이 완료되었습니다!\")\n",
        "print(\"\\n생성된 파일:\")\n",
        "print(\"  1. training_history.png - 학습 곡선\")\n",
        "print(\"  2. predictions_visualization.png - 36개 샘플 예측 결과\")\n",
        "print(\"  3. confusion_matrix.png - 혼동 행렬\")\n",
        "print(\"  4. mnist_cnn_model.pth - 학습된 모델\")"
      ],
      "metadata": {
        "id": "zne-ymR37CHg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}