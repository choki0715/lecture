{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOK1P3/VWiIqLF7w7gjpTNQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/choki0715/lecture/blob/master/quant_perform.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDMpW56_0KpY"
      },
      "outputs": [],
      "source": [
        "# answer_quality_comparison.py\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "import json\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score as bert_score\n",
        "import pandas as pd\n",
        "from typing import List, Dict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class QualityEvaluator:\n",
        "    \"\"\"ì–‘ìí™” ëª¨ë¸ ë‹µë³€ í’ˆì§ˆ í‰ê°€\"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"gpt2\"):\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.results = {}\n",
        "\n",
        "    def load_models(self):\n",
        "        \"\"\"ë‹¤ì–‘í•œ ì–‘ìí™” ëª¨ë¸ ë¡œë“œ\"\"\"\n",
        "        models = {}\n",
        "\n",
        "        # FP16 (ê¸°ì¤€ ëª¨ë¸)\n",
        "        print(\"Loading FP16 model...\")\n",
        "        models['FP16'] = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        # INT8\n",
        "        print(\"Loading INT8 model...\")\n",
        "        models['INT8'] = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            load_in_8bit=True,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        # INT4\n",
        "        print(\"Loading INT4 model...\")\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16\n",
        "        )\n",
        "        models['INT4'] = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        return models\n",
        "\n",
        "    def generate_text(self, model, prompt, max_length=100, num_samples=1, temperature=0.7):\n",
        "        \"\"\"í…ìŠ¤íŠ¸ ìƒì„±\"\"\"\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
        "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_length,\n",
        "                temperature=temperature,\n",
        "                do_sample=True if temperature > 0 else False,\n",
        "                num_return_sequences=num_samples,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        generated_texts = []\n",
        "        for output in outputs:\n",
        "            text = self.tokenizer.decode(output[inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "            generated_texts.append(text)\n",
        "\n",
        "        return generated_texts[0] if num_samples == 1 else generated_texts\n",
        "\n",
        "    def test_factual_accuracy(self, models):\n",
        "        \"\"\"ì‚¬ì‹¤ ì •í™•ë„ í…ŒìŠ¤íŠ¸\"\"\"\n",
        "        print(\"\\nğŸ“Š ì‚¬ì‹¤ ì •í™•ë„ í…ŒìŠ¤íŠ¸\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        factual_prompts = [\n",
        "            (\"The capital of France is\", [\"Paris\"]),\n",
        "            (\"Water boils at\", [\"100\", \"degrees\", \"Celsius\", \"212\", \"Fahrenheit\"]),\n",
        "            (\"The largest planet in our solar system is\", [\"Jupiter\"]),\n",
        "            (\"The speed of light is approximately\", [\"299\", \"300\", \"thousand\", \"kilometers\"]),\n",
        "            (\"The author of Romeo and Juliet is\", [\"Shakespeare\", \"William\"]),\n",
        "            (\"The chemical symbol for gold is\", [\"Au\"]),\n",
        "            (\"The square root of 144 is\", [\"12\", \"twelve\"]),\n",
        "            (\"The currency of Japan is\", [\"yen\", \"Yen\", \"JPY\"]),\n",
        "            (\"The smallest prime number is\", [\"2\", \"two\"]),\n",
        "            (\"The freezing point of water is\", [\"0\", \"zero\", \"32\", \"Celsius\", \"Fahrenheit\"])\n",
        "        ]\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        for model_name, model in models.items():\n",
        "            correct = 0\n",
        "            total = len(factual_prompts)\n",
        "\n",
        "            print(f\"\\n{model_name}:\")\n",
        "            for prompt, expected_words in factual_prompts:\n",
        "                generated = self.generate_text(model, prompt, max_length=20, temperature=0.1)\n",
        "\n",
        "                # ì •ë‹µ ì²´í¬\n",
        "                is_correct = any(word.lower() in generated.lower() for word in expected_words)\n",
        "                if is_correct:\n",
        "                    correct += 1\n",
        "                    print(f\"  âœ… {prompt} â†’ {generated[:50]}\")\n",
        "                else:\n",
        "                    print(f\"  âŒ {prompt} â†’ {generated[:50]}\")\n",
        "\n",
        "            accuracy = correct / total\n",
        "            results[model_name] = accuracy\n",
        "            print(f\"  ì •í™•ë„: {accuracy:.1%}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def test_consistency(self, models):\n",
        "        \"\"\"ì¼ê´€ì„± í…ŒìŠ¤íŠ¸ (ê°™ì€ í”„ë¡¬í”„íŠ¸, ì—¬ëŸ¬ ë²ˆ ìƒì„±)\"\"\"\n",
        "        print(\"\\nğŸ“Š ì¼ê´€ì„± í…ŒìŠ¤íŠ¸\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        test_prompt = \"The most important quality of a leader is\"\n",
        "        num_generations = 5\n",
        "\n",
        "        consistency_scores = {}\n",
        "\n",
        "        for model_name, model in models.items():\n",
        "            print(f\"\\n{model_name}:\")\n",
        "\n",
        "            generations = []\n",
        "            for i in range(num_generations):\n",
        "                text = self.generate_text(model, test_prompt, max_length=50, temperature=0.3)\n",
        "                generations.append(text)\n",
        "                print(f\"  {i+1}: {text[:80]}\")\n",
        "\n",
        "            # ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚° (ROUGE-L ì‚¬ìš©)\n",
        "            scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "            scores = []\n",
        "\n",
        "            for i in range(len(generations)):\n",
        "                for j in range(i+1, len(generations)):\n",
        "                    score = scorer.score(generations[i], generations[j])\n",
        "                    scores.append(score['rougeL'].fmeasure)\n",
        "\n",
        "            consistency_score = np.mean(scores) if scores else 0\n",
        "            consistency_scores[model_name] = consistency_score\n",
        "            print(f\"  ì¼ê´€ì„± ì ìˆ˜: {consistency_score:.3f}\")\n",
        "\n",
        "        return consistency_scores\n",
        "\n",
        "    def test_coherence(self, models):\n",
        "        \"\"\"ë…¼ë¦¬ì  ì¼ê´€ì„± í…ŒìŠ¤íŠ¸\"\"\"\n",
        "        print(\"\\nğŸ“Š ë…¼ë¦¬ì  ì¼ê´€ì„± í…ŒìŠ¤íŠ¸\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        coherence_prompts = [\n",
        "            \"If it's raining outside, you should take an umbrella because\",\n",
        "            \"To bake a cake, first you need to\",\n",
        "            \"The student failed the exam, therefore\",\n",
        "            \"Exercise is important for health because\",\n",
        "            \"Before going to sleep, it's good to\"\n",
        "        ]\n",
        "\n",
        "        coherence_scores = {}\n",
        "\n",
        "        for model_name, model in models.items():\n",
        "            print(f\"\\n{model_name}:\")\n",
        "            logical_scores = []\n",
        "\n",
        "            for prompt in coherence_prompts:\n",
        "                generated = self.generate_text(model, prompt, max_length=50, temperature=0.5)\n",
        "                print(f\"  {prompt[:30]}... â†’ {generated[:60]}\")\n",
        "\n",
        "                # ê°„ë‹¨í•œ ë…¼ë¦¬ì„± ì²´í¬ (í‚¤ì›Œë“œ ê¸°ë°˜)\n",
        "                logical_keywords = {\n",
        "                    \"raining\": [\"wet\", \"dry\", \"protect\", \"water\", \"rain\"],\n",
        "                    \"bake\": [\"ingredients\", \"mix\", \"oven\", \"recipe\", \"prepare\"],\n",
        "                    \"failed\": [\"study\", \"try\", \"next\", \"improve\", \"learn\"],\n",
        "                    \"Exercise\": [\"health\", \"body\", \"fitness\", \"energy\", \"strong\"],\n",
        "                    \"sleep\": [\"relax\", \"rest\", \"calm\", \"routine\", \"peaceful\"]\n",
        "                }\n",
        "\n",
        "                for key, keywords in logical_keywords.items():\n",
        "                    if key in prompt:\n",
        "                        score = 1.0 if any(kw in generated.lower() for kw in keywords) else 0.0\n",
        "                        logical_scores.append(score)\n",
        "                        break\n",
        "\n",
        "            coherence_scores[model_name] = np.mean(logical_scores)\n",
        "            print(f\"  ë…¼ë¦¬ì„± ì ìˆ˜: {coherence_scores[model_name]:.2f}\")\n",
        "\n",
        "        return coherence_scores\n",
        "\n",
        "    def test_creativity(self, models):\n",
        "        \"\"\"ì°½ì˜ì„± í…ŒìŠ¤íŠ¸ (ë‹¤ì–‘ì„± ì¸¡ì •)\"\"\"\n",
        "        print(\"\\nğŸ“Š ì°½ì˜ì„±/ë‹¤ì–‘ì„± í…ŒìŠ¤íŠ¸\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        creative_prompt = \"In a world where gravity works backwards,\"\n",
        "        num_samples = 5\n",
        "\n",
        "        diversity_scores = {}\n",
        "\n",
        "        for model_name, model in models.items():\n",
        "            print(f\"\\n{model_name}:\")\n",
        "\n",
        "            generations = []\n",
        "            unique_words = set()\n",
        "\n",
        "            for i in range(num_samples):\n",
        "                text = self.generate_text(model, creative_prompt, max_length=60, temperature=0.9)\n",
        "                generations.append(text)\n",
        "                unique_words.update(text.lower().split())\n",
        "                print(f\"  {i+1}: {text[:80]}\")\n",
        "\n",
        "            # ë‹¤ì–‘ì„± ì ìˆ˜ (unique words / total words)\n",
        "            total_words = sum(len(g.split()) for g in generations)\n",
        "            diversity_score = len(unique_words) / total_words if total_words > 0 else 0\n",
        "            diversity_scores[model_name] = diversity_score\n",
        "            print(f\"  ë‹¤ì–‘ì„± ì ìˆ˜: {diversity_score:.3f}\")\n",
        "\n",
        "        return diversity_scores\n",
        "\n",
        "    def test_perplexity(self, models):\n",
        "        \"\"\"Perplexity ì¸¡ì • (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)\"\"\"\n",
        "        print(\"\\nğŸ“Š Perplexity ì¸¡ì •\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # WikiText-2 ë°ì´í„°ì…‹ ìƒ˜í”Œ ì‚¬ìš©\n",
        "        test_texts = [\n",
        "            \"The history of artificial intelligence began in antiquity, with myths, stories and rumors of artificial beings endowed with intelligence.\",\n",
        "            \"Machine learning is a method of data analysis that automates analytical model building.\",\n",
        "            \"Natural language processing is a subfield of linguistics, computer science, and artificial intelligence.\",\n",
        "            \"Deep learning is part of a broader family of machine learning methods based on artificial neural networks.\",\n",
        "            \"Computer vision is an interdisciplinary scientific field that deals with how computers gain understanding from digital images.\"\n",
        "        ]\n",
        "\n",
        "        perplexity_scores = {}\n",
        "\n",
        "        for model_name, model in models.items():\n",
        "            total_loss = 0\n",
        "            total_tokens = 0\n",
        "\n",
        "            for text in test_texts:\n",
        "                inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "                inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(**inputs, labels=inputs['input_ids'])\n",
        "                    total_loss += outputs.loss.item() * inputs['input_ids'].shape[1]\n",
        "                    total_tokens += inputs['input_ids'].shape[1]\n",
        "\n",
        "            perplexity = np.exp(total_loss / total_tokens)\n",
        "            perplexity_scores[model_name] = perplexity\n",
        "            print(f\"{model_name}: {perplexity:.2f}\")\n",
        "\n",
        "        return perplexity_scores\n",
        "\n",
        "    def test_qa_capability(self, models):\n",
        "        \"\"\"ì§ˆë¬¸-ë‹µë³€ ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸\"\"\"\n",
        "        print(\"\\nğŸ“Š Q&A ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        qa_pairs = [\n",
        "            (\"Q: What is 15 + 27?\\nA:\", [\"42\"]),\n",
        "            (\"Q: What color is the sky?\\nA:\", [\"blue\", \"Blue\"]),\n",
        "            (\"Q: How many days are in a week?\\nA:\", [\"7\", \"seven\", \"Seven\"]),\n",
        "            (\"Q: What is the opposite of hot?\\nA:\", [\"cold\", \"Cold\"]),\n",
        "            (\"Q: What animal is known as man's best friend?\\nA:\", [\"dog\", \"Dog\"])\n",
        "        ]\n",
        "\n",
        "        qa_scores = {}\n",
        "\n",
        "        for model_name, model in models.items():\n",
        "            correct = 0\n",
        "            print(f\"\\n{model_name}:\")\n",
        "\n",
        "            for question, expected in qa_pairs:\n",
        "                answer = self.generate_text(model, question, max_length=20, temperature=0.1)\n",
        "                is_correct = any(exp in answer for exp in expected)\n",
        "\n",
        "                if is_correct:\n",
        "                    correct += 1\n",
        "                    print(f\"  âœ… {question.split('?')[0]}? â†’ {answer[:30]}\")\n",
        "                else:\n",
        "                    print(f\"  âŒ {question.split('?')[0]}? â†’ {answer[:30]}\")\n",
        "\n",
        "            qa_scores[model_name] = correct / len(qa_pairs)\n",
        "            print(f\"  ì •ë‹µë¥ : {qa_scores[model_name]:.1%}\")\n",
        "\n",
        "        return qa_scores\n",
        "\n",
        "    def run_all_tests(self):\n",
        "        \"\"\"ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰\"\"\"\n",
        "        print(\"ğŸš€ ì–‘ìí™” ëª¨ë¸ ë‹µë³€ í’ˆì§ˆ ì¢…í•© í‰ê°€\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # ëª¨ë¸ ë¡œë“œ\n",
        "        models = self.load_models()\n",
        "\n",
        "        # ê° í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
        "        results = {\n",
        "            'factual_accuracy': self.test_factual_accuracy(models),\n",
        "            'consistency': self.test_consistency(models),\n",
        "            'coherence': self.test_coherence(models),\n",
        "            'creativity': self.test_creativity(models),\n",
        "            'perplexity': self.test_perplexity(models),\n",
        "            'qa_capability': self.test_qa_capability(models)\n",
        "        }\n",
        "\n",
        "        # ê²°ê³¼ ìš”ì•½\n",
        "        self.print_summary(results)\n",
        "\n",
        "        # CSVë¡œ ì €ì¥\n",
        "        self.save_results(results)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def print_summary(self, results):\n",
        "        \"\"\"ê²°ê³¼ ìš”ì•½ ì¶œë ¥\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ğŸ“Š ì¢…í•© í‰ê°€ ê²°ê³¼\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # DataFrame ìƒì„±\n",
        "        df = pd.DataFrame(results)\n",
        "\n",
        "        # ì ìˆ˜ ì •ê·œí™” (PerplexityëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìœ¼ë¯€ë¡œ ì—­ìˆ˜)\n",
        "        df_normalized = df.copy()\n",
        "        df_normalized['perplexity'] = 1 / df_normalized['perplexity']\n",
        "\n",
        "        # ì¢…í•© ì ìˆ˜ ê³„ì‚°\n",
        "        df_normalized['total_score'] = df_normalized.mean(axis=1)\n",
        "\n",
        "        print(\"\\nğŸ“ˆ ê° í•­ëª©ë³„ ì ìˆ˜:\")\n",
        "        print(df.round(3).to_string())\n",
        "\n",
        "        print(\"\\nğŸ† ì¢…í•© ìˆœìœ„:\")\n",
        "        ranking = df_normalized['total_score'].sort_values(ascending=False)\n",
        "        for i, (model, score) in enumerate(ranking.items(), 1):\n",
        "            print(f\"{i}. {model}: {score:.3f}\")\n",
        "\n",
        "        # FP16 ëŒ€ë¹„ í’ˆì§ˆ ì €í•˜ìœ¨\n",
        "        print(\"\\nğŸ“‰ FP16 ëŒ€ë¹„ í’ˆì§ˆ ë³€í™”:\")\n",
        "        baseline = df.loc['FP16']\n",
        "        for model in df.index:\n",
        "            if model != 'FP16':\n",
        "                diff = df.loc[model] - baseline\n",
        "                avg_diff = diff.mean()\n",
        "                print(f\"{model}: {avg_diff:+.3f} (í‰ê· )\")\n",
        "\n",
        "    def save_results(self, results):\n",
        "        \"\"\"ê²°ê³¼ ì €ì¥\"\"\"\n",
        "        df = pd.DataFrame(results)\n",
        "        df.to_csv('quality_evaluation_results.csv')\n",
        "\n",
        "        # JSONìœ¼ë¡œë„ ì €ì¥\n",
        "        with open('quality_evaluation_results.json', 'w') as f:\n",
        "            json.dump(results, f, indent=2)\n",
        "\n",
        "        print(\"\\nğŸ’¾ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:\")\n",
        "        print(\"  - quality_evaluation_results.csv\")\n",
        "        print(\"  - quality_evaluation_results.json\")\n",
        "\n",
        "# ì‹¤í–‰\n",
        "if __name__ == \"__main__\":\n",
        "    evaluator = QualityEvaluator(model_name=\"gpt2\")\n",
        "    results = evaluator.run_all_tests()"
      ]
    }
  ]
}