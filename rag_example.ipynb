{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIAxuVqFPnuyH4RrHHqPX4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/choki0715/lecture/blob/master/rag_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNvxq2r8ucjl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# 임베딩 메소드 설정\n",
        "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "# from langchain.embeddings import FastEmbedEmbeddings\n",
        "# from langchain_community.embeddings import OllamaEmbeddings\n",
        "# from langchain_community.embeddings import GPT4AllEmbeddings\n",
        "# from gpt4all import Embed4All\n",
        "# from langchain.embeddings import OpenAIEmbeddings\n",
        "# from langchain_community.embeddings import OpenAIEmbeddings\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "# from langchain.embeddings import HuggingFaceEmbeddings\n",
        "# from langchain_mistralai import MistralAIEmbeddings\n",
        "# from langchain_google_vertexai import VertexAIEmbeddings\n",
        "\n",
        "# PDF 리더\n",
        "# from langchain.document_loaders import PyPDFLoader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "# from langchain.vectorstores import FAISS\n",
        "\n",
        "# 벡터 데이터 베이스 결정\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# 언어모델 결정\n",
        "from langchain_community.llms import Ollama\n",
        "# from langchain.llms import OpenAI\n",
        "# from langchain_community.llms import OpenAI\n",
        "# from langchain_community.chat_models import ChatOpenAI\n",
        "# from langchain.chat_models import ChatOllama\n",
        "\n",
        "# 대화 프롬프트 메모리 관리\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# 스트림릿 정의\n",
        "import streamlit as st\n",
        "\n",
        "import tempfile\n",
        "\n",
        "os.environ['OPENAI_API_KEY']= \"\"\n",
        "\n",
        "###########################################################################\n",
        "# streamlit UI\n",
        "# webpage 규격 및 디자인\n",
        "st.set_page_config(layout=\"wide\")\n",
        "st.markdown(\"<h1 style='text-align: center; color: gray;'>인구감소 대응 정책을 위한 RAG System</h1>\", unsafe_allow_html=True)\n",
        "st.markdown(\"<h2 style='text-align: center; color: gray;'> Chat with PDFs that you upload </h2>\", unsafe_allow_html=True)\n",
        "st.markdown(\"<h5 style='text-align: center; color: gray;'>오픈소스 ollama LLM 기반</h5>\", unsafe_allow_html=True)\n",
        "st.markdown(\"<h3 style='text-align: center; color: gray;'> AIDENTIFY Inc.</h3>\", unsafe_allow_html=True)\n",
        "\n",
        "# streamlit UI\n",
        "###########################################################################\n",
        "\n",
        "# # vector data 보기\n",
        "# def show_vstore(vstore):\n",
        "#   vector_df = store_to_df(vstore)\n",
        "#   display(vector_df)\n",
        "\n",
        "# #vector database 구축\n",
        "# def store_to_df(vstore):\n",
        "#   v_dict = vstore.docstore._dict\n",
        "#   data_rows = []\n",
        "#   for k in v_dict.keys():\n",
        "#     doc_name = v_dict[k].metadata['source'].split('/')[-1]\n",
        "#     page_number = doc_name.split('_')[-1]\n",
        "#     content = v_dict[k].page_content\n",
        "#     data_rows.append({\"chunk_id\":k, \"document\":doc_name, \"page\":page_number, \"content\":content})\n",
        "#   vector_df = pd.DataFrame(data_rows)\n",
        "#   return vector_df\n",
        "\n",
        "def document_data(query, chat_history, vectorstore):\n",
        "\n",
        "   # Loading the saved embeddings\n",
        "   # new_vector_store =FAISS.load_local(\"vectors\", embeddings)\n",
        "\n",
        "    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "\n",
        "   #  llm 선정\n",
        "    llm = Ollama(model=\"llama3\")\n",
        "   #  llm = Ollama(model=\"solar\")\n",
        "   #  llm = Ollama(model=\"qwen2\")\n",
        "\n",
        "    # langchain의 핵심\n",
        "    qna = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    memory=memory)\n",
        "\n",
        "    return qna({\"question\":query, \"chat_history\":chat_history})\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "   #  임베딩 메소드 결정\n",
        "   #  embeddings = Embed4All()\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "   #  embeddings = OpenAIEmbeddings()\n",
        "   #  embeddings = HuggingFaceEmbeddings()\n",
        "\n",
        "    # Load Data for Retrieved Prompt\n",
        "    uploaded_files = st.file_uploader(\"Choose a PDF file\", type=\"PDF\", accept_multiple_files=True)\n",
        "\n",
        "    # chunk 스플릿터 정의\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap= 100, separators=[\"\\n\\n\",\"\\n\",\" \",\"\"])\n",
        "\n",
        "    temp_dir = tempfile.TemporaryDirectory()\n",
        "    docs = []\n",
        "\n",
        "    for i, uploaded_file in enumerate(uploaded_files):\n",
        "      #bytes_data = uploaded_file.read()\n",
        "      st.write(\"filename:\", i, uploaded_file.name)\n",
        "      temp_filepath = os.path.join(temp_dir.name, uploaded_file.name)\n",
        "      with open(temp_filepath, \"wb\") as f:\n",
        "            f.write(uploaded_file.getvalue())\n",
        "      #st.write(bytes_data)\n",
        "      loader = PyPDFLoader(temp_filepath)\n",
        "      docs.extend(loader.load())\n",
        "      # doc = loader.load()\n",
        "\n",
        "      split_text = text_splitter.split_documents(documents = docs)\n",
        "      vectorstore = FAISS.from_documents(split_text, embedding=embeddings)\n",
        "\n",
        "    # 벡터데이터베이스 구축 확인\n",
        "    print(\"Embeddings successfully saved in vector Database and saved locally\")\n",
        "\n",
        "    # ChatInput\n",
        "    st.header(' ')\n",
        "    st.header(' ')\n",
        "    st.header(':blue[질문을 입력해 주세요]', divider='rainbow')\n",
        "    #st.header('_Streamlit_ is :blue[cool] :sunglasses:')\n",
        "    prompt = st.chat_input(\"Enter your questions here\")\n",
        "\n",
        "    if \"user_prompt_history\" not in st.session_state:\n",
        "       st.session_state[\"user_prompt_history\"]=[]\n",
        "    if \"chat_answers_history\" not in st.session_state:\n",
        "       st.session_state[\"chat_answers_history\"]=[]\n",
        "    if \"chat_history\" not in st.session_state:\n",
        "       st.session_state[\"chat_history\"]=[]\n",
        "\n",
        "    if prompt:\n",
        "       with st.spinner(\"Generating......\"):\n",
        "           output=document_data(query=prompt, chat_history = st.session_state[\"chat_history\"], vectorstore=vectorstore)\n",
        "\n",
        "          # Storing the questions, answers and chat history\n",
        "\n",
        "           st.session_state[\"chat_answers_history\"].append(output['answer'])\n",
        "           st.session_state[\"user_prompt_history\"].append(prompt)\n",
        "           st.session_state[\"chat_history\"].append((prompt,output['answer']))\n",
        "\n",
        "    # Displaying the chat history\n",
        "\n",
        "    if st.session_state[\"chat_answers_history\"]:\n",
        "       for i, j in zip(st.session_state[\"chat_answers_history\"],st.session_state[\"user_prompt_history\"]):\n",
        "          message1 = st.chat_message(\"user\")\n",
        "          message1.write(j)\n",
        "          message2 = st.chat_message(\"assistant\")\n",
        "          message2.write(i)\n",
        "\n"
      ]
    }
  ]
}