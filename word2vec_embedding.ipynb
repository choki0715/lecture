{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1CSRGBykXG6xxHvnqwcr6uzYKSH0t-xi2",
      "authorship_tag": "ABX9TyPzD167+/LybFNCYxPmL2d5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/choki0715/lecture/blob/master/word2vec_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giqO4FlXpnCE",
        "outputId": "cb8f1d5b-2992-4415-d4dc-f42cae1bfb7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['개 한 마리가 입에 고깃점(고기 한 조각)을 물고서 개울 위 다리를 건너다 물 위에 비친 자신의 그림자를 보고서 그게 자기 것보다 두 배가 큰 고깃점을 문 다른 개인 걸로 착각을 하고 말았어요. 그 즉시 개가 물고 있던 고깃점을 떨어뜨리곤 사납게 다른 개에게로 달려들고 말았어요',\n",
            "       ' 그 개에게서 자기 거보다 더 큰 고깃점을 빼앗기 위해서였죠. 그리하여 개는 둘 다를 잃고 말았답니다. 그가 물로 뛰어들어 왈칵 붙잡은 건 그림자였고',\n",
            "       ' 그리고 자신이 원래 물고 있던 그 고깃점은 개울에 그냥 유유히 떠내려가고 말았으니까요.'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim.models.fasttext import FastText\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from gensim.test.utils import datapath\n",
        "import pandas as pd\n",
        "\n",
        "path = '/content/drive/MyDrive/a_poor_dog.txt'\n",
        "content = pd.read_csv(path)\n",
        "content = str(content.columns)\n",
        "print(content)\n",
        "type(content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "#nltk.download('punkt')"
      ],
      "metadata": {
        "id": "hfXUzOhZzvgV"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sentences = word_tokenize(content)\n",
        "print(sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IK5B7X0B6bdi",
        "outputId": "28489d12-0504-49e2-92bc-e731f7d53c27"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Index', '(', '[', \"'\", '개', '한', '마리가', '입에', '고깃점', '(', '고기', '한', '조각', ')', '을', '물고서', '개울', '위', '다리를', '건너다', '물', '위에', '비친', '자신의', '그림자를', '보고서', '그게', '자기', '것보다', '두', '배가', '큰', '고깃점을', '문', '다른', '개인', '걸로', '착각을', '하고', '말았어요', '.', '그', '즉시', '개가', '물고', '있던', '고깃점을', '떨어뜨리곤', '사납게', '다른', '개에게로', '달려들고', '말았어요', \"'\", ',', \"'\", '그', '개에게서', '자기', '거보다', '더', '큰', '고깃점을', '빼앗기', '위해서였죠', '.', '그리하여', '개는', '둘', '다를', '잃고', '말았답니다', '.', '그가', '물로', '뛰어들어', '왈칵', '붙잡은', '건', '그림자였고', \"'\", ',', \"'\", '그리고', '자신이', '원래', '물고', '있던', '그', '고깃점은', '개울에', '그냥', '유유히', '떠내려가고', '말았으니까요', '.', \"'\", ']', ',', \"dtype='object\", \"'\", ')']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = FastText(sentences, min_count=1, vector_size=100, window=10)\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4ziNc3l6yYF",
        "outputId": "179bc057-6578-4e4b-a83d-959a17c0014b"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastText<vocab=113, vector_size=100, alpha=0.025>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = gensim.models.word2vec.Text8Corpus(path)\n",
        "model = Word2Vec(sentences, min_count = 10, vector_size= 50, window = 5)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "YfuyD2eOC66U",
        "outputId": "6583a9f6-936d-40e8-9cb1-82538098fd92"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-c230a85e20f8>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mText8Corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             self.train(\n\u001b[0m\u001b[1;32m    431\u001b[0m                 \u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_training_sanity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1046\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_check_training_sanity\u001b[0;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_to_index\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# should be set by `build_vocab`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1554\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you must first build vocabulary before training the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1555\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you must initialize vectors before training the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('fasttext_model')\n",
        "my_model = FastText.load('fasttext_model')\n",
        "my_model.wv['개울']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDiUn7hXtUea",
        "outputId": "97e9809c-d90f-455e-8dba-090cfc865b40"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 5.7323086e-03,  1.1066467e-03, -2.6969130e-03,  3.1539770e-03,\n",
              "       -1.2316921e-03, -4.9525560e-03,  8.5229921e-04, -2.8677129e-03,\n",
              "       -5.7947226e-03, -1.3629940e-03, -1.4695534e-03,  4.3500448e-03,\n",
              "        4.0836968e-03, -5.4147880e-04,  6.3145994e-03, -1.9989216e-03,\n",
              "        1.1729015e-03, -1.8987985e-03,  1.3888498e-03, -1.6451596e-04,\n",
              "        2.1260132e-03,  4.6154936e-03,  2.5977129e-03, -2.0403184e-03,\n",
              "        1.1775981e-03, -1.9880105e-03, -2.8650695e-03, -5.9851333e-03,\n",
              "        2.2617206e-03, -3.3789808e-03,  9.8624593e-04,  6.1008073e-03,\n",
              "        8.9554861e-04, -3.3248353e-03, -2.1354833e-03,  4.5556654e-03,\n",
              "        5.0040199e-03, -3.2826799e-03,  2.3188691e-03, -3.3956424e-03,\n",
              "       -2.2517575e-03,  1.9093514e-03, -2.1135123e-05, -1.2270227e-03,\n",
              "       -2.0744149e-03,  2.0784230e-03,  3.2760247e-03, -1.5488343e-03,\n",
              "        6.1653648e-03,  8.8869281e-05,  3.8184572e-03,  2.0556578e-03,\n",
              "        1.8906383e-03, -2.6777282e-03, -1.1976198e-03, -6.1068055e-04,\n",
              "       -2.9727172e-03,  1.8015457e-04, -4.6694889e-03, -3.5807334e-05,\n",
              "       -2.7339018e-03, -5.8007392e-04, -1.2294328e-03,  5.2562286e-03,\n",
              "        7.5666620e-03, -4.5596906e-03, -6.3064122e-03, -4.6258792e-03,\n",
              "        4.3514185e-03, -9.1465736e-05, -3.7338475e-03,  9.3537696e-05,\n",
              "        5.8362857e-03,  2.1698528e-03,  1.9608911e-03, -5.0907712e-03,\n",
              "       -2.4060186e-03, -2.8952213e-03,  2.4883051e-03,  4.1388418e-03,\n",
              "        6.0231220e-03,  6.1835796e-03, -3.1757697e-03, -1.6501192e-03,\n",
              "       -1.6192874e-03,  7.9481601e-04, -3.9668619e-03, -2.5854968e-03,\n",
              "       -1.6507991e-03,  1.2449952e-03, -5.2595302e-04,  2.7411366e-03,\n",
              "        2.6387393e-03, -3.0788034e-03, -2.5382158e-04, -4.3060598e-03,\n",
              "       -2.3506519e-04,  1.6711453e-03, -9.6546113e-04,  3.7959038e-05],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_model.wv.similarity('개', '고기')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmrEK39Ku04Q",
        "outputId": "c8631d22-4248-46b7-b50e-aa09664e68b6"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.096512035"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_model.wv.most_similar('다리를', topn=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tm098CSv9hVO",
        "outputId": "d0a39bd1-f2a6-45d5-e855-8d09ea54b9c0"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('이', 0.3082560896873474),\n",
              " ('은', 0.2529433071613312),\n",
              " ('로', 0.2068037986755371),\n",
              " ('둘', 0.13516917824745178),\n",
              " ('건', 0.13260558247566223),\n",
              " ('내', 0.12850236892700195),\n",
              " ('울', 0.1257200837135315),\n",
              " ('I', 0.11669730395078659),\n",
              " ('착', 0.11604443937540054),\n",
              " ('였', 0.1082601472735405),\n",
              " ('y', 0.10623941570520401),\n",
              " ('거', 0.09825176000595093),\n",
              " ('곤', 0.09817858785390854),\n",
              " ('를', 0.09793931990861893),\n",
              " ('C', 0.09724832326173782)]"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yKwqvQjh_OBs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}