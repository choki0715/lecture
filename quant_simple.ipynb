{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+sGTJ7H6rQeJVAdGm6igF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/choki0715/lecture/blob/master/quant_simple.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kmeQ6WRoriU"
      },
      "outputs": [],
      "source": [
        "# accurate_size_comparison.py\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "import gc\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "def get_model_size_mb(model):\n",
        "    \"\"\"ëª¨ë¸ì˜ ì‹¤ì œ ë©”ëª¨ë¦¬ í¬ê¸° ê³„ì‚°\"\"\"\n",
        "    param_size = 0\n",
        "    buffer_size = 0\n",
        "\n",
        "    for param in model.parameters():\n",
        "        # BitsAndBytes ì–‘ìí™”ëœ ê²½ìš° ì‹¤ì œ ë¹„íŠ¸ ìˆ˜ ê³ ë ¤\n",
        "        if hasattr(param, 'quant_state'):\n",
        "            # 4-bit or 8-bit ì–‘ìí™”ëœ íŒŒë¼ë¯¸í„°\n",
        "            bits = 4 if '4bit' in str(param.dtype) else 8\n",
        "            param_size += param.numel() * bits / 8  # bits to bytes\n",
        "        else:\n",
        "            param_size += param.numel() * param.element_size()\n",
        "\n",
        "    for buffer in model.buffers():\n",
        "        buffer_size += buffer.numel() * buffer.element_size()\n",
        "\n",
        "    return (param_size + buffer_size) / (1024 * 1024)\n",
        "\n",
        "def measure_gpu_memory():\n",
        "    \"\"\"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        return torch.cuda.memory_allocated() / (1024 * 1024)\n",
        "    return 0\n",
        "\n",
        "def compare_quantization():\n",
        "    \"\"\"ì–‘ìí™” ì „í›„ í¬ê¸° ë¹„êµ\"\"\"\n",
        "    print(\"ğŸ”¬ ì •í™•í•œ ëª¨ë¸ í¬ê¸° ë¹„êµ\\n\")\n",
        "\n",
        "    model_name = \"gpt2\"\n",
        "\n",
        "    # 1. ì›ë³¸ ëª¨ë¸\n",
        "    print(\"=\"*50)\n",
        "    print(\"1. ì›ë³¸ ëª¨ë¸ (FP32)\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # GPU ë©”ëª¨ë¦¬ ì´ˆê¸°í™”\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    model_fp32 = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    # CPU ë©”ëª¨ë¦¬ ì¸¡ì •\n",
        "    fp32_params = sum(p.numel() for p in model_fp32.parameters())\n",
        "    fp32_size = sum(p.numel() * p.element_size() for p in model_fp32.parameters()) / (1024**2)\n",
        "\n",
        "    print(f\"ğŸ“Š íŒŒë¼ë¯¸í„° ìˆ˜: {fp32_params:,}\")\n",
        "    print(f\"ğŸ“Š ì´ë¡ ì  í¬ê¸°: {fp32_size:.2f} MB\")\n",
        "\n",
        "    # ì‹¤ì œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰\n",
        "    process = psutil.Process(os.getpid())\n",
        "    fp32_memory = process.memory_info().rss / (1024 * 1024)\n",
        "    print(f\"ğŸ“Š ì‹¤ì œ RAM ì‚¬ìš©: {fp32_memory:.2f} MB\")\n",
        "\n",
        "    del model_fp32\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # 2. 8-bit ëª¨ë¸\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"2. 8-bit ì–‘ìí™” ëª¨ë¸\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    model_8bit = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        load_in_8bit=True,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    # ì´ë¡ ì  í¬ê¸° (8-bit)\n",
        "    expected_8bit = fp32_size / 4  # 32bit -> 8bit\n",
        "    print(f\"ğŸ“Š ì˜ˆìƒ í¬ê¸°: {expected_8bit:.2f} MB\")\n",
        "\n",
        "    # GPU ë©”ëª¨ë¦¬ ì¸¡ì •\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_8bit = torch.cuda.memory_allocated() / (1024**2)\n",
        "        print(f\"ğŸ® GPU ë©”ëª¨ë¦¬: {gpu_8bit:.2f} MB\")\n",
        "        print(f\"ğŸ“‰ GPU ê°ì†Œìœ¨: {(1 - gpu_8bit/fp32_size)*100:.1f}%\")\n",
        "\n",
        "    # RAM ì¸¡ì •\n",
        "    process = psutil.Process(os.getpid())\n",
        "    ram_8bit = process.memory_info().rss / (1024 * 1024)\n",
        "    print(f\"ğŸ“Š RAM ì‚¬ìš©: {ram_8bit:.2f} MB\")\n",
        "\n",
        "    del model_8bit\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # 3. 4-bit ëª¨ë¸\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"3. 4-bit ì–‘ìí™” ëª¨ë¸\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    model_4bit = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    # ì´ë¡ ì  í¬ê¸° (4-bit)\n",
        "    expected_4bit = fp32_size / 8  # 32bit -> 4bit\n",
        "    print(f\"ğŸ“Š ì˜ˆìƒ í¬ê¸°: {expected_4bit:.2f} MB\")\n",
        "\n",
        "    # GPU ë©”ëª¨ë¦¬ ì¸¡ì •\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_4bit = torch.cuda.memory_allocated() / (1024**2)\n",
        "        print(f\"ğŸ® GPU ë©”ëª¨ë¦¬: {gpu_4bit:.2f} MB\")\n",
        "        print(f\"ğŸ“‰ GPU ê°ì†Œìœ¨: {(1 - gpu_4bit/fp32_size)*100:.1f}%\")\n",
        "\n",
        "    # RAM ì¸¡ì •\n",
        "    process = psutil.Process(os.getpid())\n",
        "    ram_4bit = process.memory_info().rss / (1024 * 1024)\n",
        "    print(f\"ğŸ“Š RAM ì‚¬ìš©: {ram_4bit:.2f} MB\")\n",
        "\n",
        "    del model_4bit\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # 4. ìš”ì•½\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ğŸ“Š ìš”ì•½\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"FP32: {fp32_size:.2f} MB (ê¸°ì¤€)\")\n",
        "    print(f\"INT8: ~{fp32_size/4:.2f} MB (ì´ë¡ ì  75% ê°ì†Œ)\")\n",
        "    print(f\"INT4: ~{fp32_size/8:.2f} MB (ì´ë¡ ì  87.5% ê°ì†Œ)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    compare_quantization()"
      ]
    }
  ]
}